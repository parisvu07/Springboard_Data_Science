{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf788bd",
   "metadata": {},
   "source": [
    "# 3 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85953d52",
   "metadata": {},
   "source": [
    "The objectives of this notebook is to create a machine learning capable to segmentize a given Tweet into its category. We will be using word embedding to define the clusters of the tweets. Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions.\n",
    "\n",
    "Guidance from Springboard:\n",
    "* Create dummy or indicator features for categorical variables\n",
    "* Standardize the magnitude of numeric features using a scaler\n",
    "* Split your data into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dcbe6",
   "metadata": {},
   "source": [
    "## 3.1 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a928531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
<<<<<<< HEAD
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
=======
    "import datetime\n",
>>>>>>> 409ddbd (worked on notebook 3)
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a2e4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>Volume</th>\n",
       "      <th>%_change_Open</th>\n",
       "      <th>%_change_High</th>\n",
       "      <th>%_change_Low</th>\n",
       "      <th>%_change_Close</th>\n",
       "      <th>%_change_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>114311700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>87830100</td>\n",
       "      <td>4.934514</td>\n",
       "      <td>2.201715</td>\n",
       "      <td>4.771583</td>\n",
       "      <td>2.562311</td>\n",
       "      <td>-23.166133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>79471000</td>\n",
       "      <td>-0.661926</td>\n",
       "      <td>0.793328</td>\n",
       "      <td>-0.866491</td>\n",
       "      <td>0.205326</td>\n",
       "      <td>-9.517352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>68402200</td>\n",
       "      <td>1.207739</td>\n",
       "      <td>0.108555</td>\n",
       "      <td>1.545351</td>\n",
       "      <td>-0.662562</td>\n",
       "      <td>-13.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>85925600</td>\n",
       "      <td>-2.242648</td>\n",
       "      <td>-3.009345</td>\n",
       "      <td>-3.973285</td>\n",
       "      <td>-3.671873</td>\n",
       "      <td>25.618182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time     Volume  %_change_Open  %_change_High  \\\n",
       "0  2022-10-03  00:00:00  114311700            NaN            NaN   \n",
       "1  2022-10-04  00:00:00   87830100       4.934514       2.201715   \n",
       "2  2022-10-05  00:00:00   79471000      -0.661926       0.793328   \n",
       "3  2022-10-06  00:00:00   68402200       1.207739       0.108555   \n",
       "4  2022-10-07  00:00:00   85925600      -2.242648      -3.009345   \n",
       "\n",
       "   %_change_Low  %_change_Close  %_change_Volume  \n",
       "0           NaN             NaN              NaN  \n",
       "1      4.771583        2.562311       -23.166133  \n",
       "2     -0.866491        0.205326        -9.517352  \n",
       "3      1.545351       -0.662562       -13.928100  \n",
       "4     -3.973285       -3.671873        25.618182  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing stock data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "eda_stock_data = pd.read_csv('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/eda_stock_data.csv', encoding='latin-1')\n",
    "eda_stock_data.reset_index(drop=True, inplace=True)\n",
    "eda_stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a92369a",
<<<<<<< HEAD
=======
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:52</td>\n",
       "      <td>nicrae45</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.800</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:00</td>\n",
       "      <td>0x1585D65F0</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>4ch3t3 _syco   hehe butterfly issues 2.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:34</td>\n",
       "      <td>equitydd</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2011  this is $aapl trend line that played out...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:31</td>\n",
       "      <td>THESMARR</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>can yâall give   her own pink iphone ???</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:26</td>\n",
       "      <td>_Idontknowbro_</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>ios16 is messing with my phone service yâal...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.200</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time            user  likes              source  \\\n",
       "0  2022-10-30  20:29:52        nicrae45      0  Twitter for iPhone   \n",
       "1  2022-10-30  20:29:00     0x1585D65F0      1  Twitter for iPhone   \n",
       "2  2022-10-30  20:28:34        equitydd      1  Twitter for iPhone   \n",
       "3  2022-10-30  20:28:31        THESMARR      0     Twitter Web App   \n",
       "4  2022-10-30  20:28:26  _Idontknowbro_      0  Twitter for iPhone   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0                                                lol         0.700     0.800   \n",
       "1           4ch3t3 _syco   hehe butterfly issues 2.0         0.000     0.000   \n",
       "2  2011  this is $aapl trend line that played out...         0.325     0.325   \n",
       "3         can yâall give   her own pink iphone ???         0.650     0.250   \n",
       "4   ios16 is messing with my phone service yâal...         0.600     0.200   \n",
       "\n",
       "   Analysis  \n",
       "0  Positive  \n",
       "1   Neutral  \n",
       "2  Positive  \n",
       "3  Positive  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing tweet data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "trading_hours_tweets = pd.read_csv('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/trading_hours_tweets.csv', encoding='latin-1')\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e4e55",
   "metadata": {},
   "source": [
    "## 3.2 Feature Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9943a5",
   "metadata": {},
   "source": [
    "Text Mining is the process of deriving meaningful information from natural language text. Natural Language Processing(NLP) is a part of computer science and artificial intelligence which deals with human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c29e6",
   "metadata": {},
   "source": [
    "### 3.2.1 Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336ba690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of characters present in a tweet.\n",
    "def count_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e67a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words present in each line of tweet\n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a17c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of punctuation\n",
    "import string\n",
    "def count_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cae3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words in quotation marks\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(('.'|\".\"), text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4240ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e1db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of unique words\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6456913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0724b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda51c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of stopwords\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24f218",
   "metadata": {},
   "source": [
    "### 3.2.2 Implementation of feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b07f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['char_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_chars(x))\n",
    "trading_hours_tweets['word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_words(x))\n",
    "trading_hours_tweets['sent_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_sent(x))\n",
    "trading_hours_tweets['stopword_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_stopwords(x))\n",
    "trading_hours_tweets['unique_word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_unique_words(x))\n",
    "trading_hours_tweets['htag_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_htags(x))\n",
    "trading_hours_tweets['mention_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_mentions(x))\n",
    "trading_hours_tweets['punct_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cee4ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average word length\n",
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f81658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average sentence length\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8e7e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of unique words to total word count\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6513178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of stopwords to total word count\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e1dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b359c",
   "metadata": {},
   "source": [
    "### 3.2.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca02d3",
   "metadata": {},
   "source": [
    "Here we turn our twitter strings to lists of individual tokens (words, punctuations). Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "302e93ed",
>>>>>>> 409ddbd (worked on notebook 3)
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
<<<<<<< HEAD
=======
       "      <th>char_count</th>\n",
       "      <th>...</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>avg_sentlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "      <th>tokens</th>\n",
>>>>>>> 409ddbd (worked on notebook 3)
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:10</td>\n",
       "      <td>BarbaraDarlin</td>\n",
       "      <td>0</td>\n",
<<<<<<< HEAD
       "      <td>Tweetbot for iÎS</td>\n",
       "      <td>.@apple needs to do this WORLDWIDE! https://t....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
=======
       "      <td>Twitter for iPhone</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.800</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[lol]</td>\n",
>>>>>>> 409ddbd (worked on notebook 3)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:03</td>\n",
       "      <td>Options</td>\n",
       "      <td>0</td>\n",
       "      <td>StockTwits Web</td>\n",
       "      <td>#HotOptions Report For End Of Day, October 4, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:59</td>\n",
       "      <td>IGSquawk</td>\n",
       "      <td>8</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
=======
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[4ch3t3, _syco, hehe, butterfly, issues, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:34</td>\n",
       "      <td>equitydd</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2011  this is $aapl trend line that played out...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Positive</td>\n",
       "      <td>218</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.069767</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>[2011, this, is, aapl, trend, line, that, play...</td>\n",
>>>>>>> 409ddbd (worked on notebook 3)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:54</td>\n",
       "      <td>breckyunits</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
<<<<<<< HEAD
       "      <td>I gave this talk in 2017 at @forwardJS . They ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-0.366667</td>\n",
       "      <td>Negative</td>\n",
=======
       "      <td>can yâall give   her own pink iphone ???</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>[can, yâ, all, give, her, own, pink, iphone]</td>\n",
>>>>>>> 409ddbd (worked on notebook 3)
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:11</td>\n",
       "      <td>emilesmithjoe</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
<<<<<<< HEAD
       "      <td>@Apple make a cannon emoji. Thank you</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
=======
       "      <td>ios16 is messing with my phone service yâal...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.153846</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[ios16, is, messing, with, my, phone, service,...</td>\n",
>>>>>>> 409ddbd (worked on notebook 3)
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time           user likes              source  \\\n",
       "0  2022-10-04  20:29:10  BarbaraDarlin     0   Tweetbot for iÎS   \n",
       "1  2022-10-04  20:29:03        Options     0      StockTwits Web   \n",
       "2  2022-10-04  20:27:59       IGSquawk     8           TweetDeck   \n",
       "3  2022-10-04  20:27:54    breckyunits     0     Twitter Web App   \n",
       "4  2022-10-04  20:27:11  emilesmithjoe     0  Twitter for iPhone   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0  .@apple needs to do this WORLDWIDE! https://t....      0.000000  0.000000   \n",
       "1  #HotOptions Report For End Of Day, October 4, ...      0.000000  0.000000   \n",
       "2  FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...      0.000000  0.000000   \n",
       "3  I gave this talk in 2017 at @forwardJS . They ...      0.633333 -0.366667   \n",
       "4              @Apple make a cannon emoji. Thank you      0.000000  0.000000   \n",
       "\n",
<<<<<<< HEAD
       "   Analysis  \n",
       "0   Neutral  \n",
       "1   Neutral  \n",
       "2   Neutral  \n",
       "3  Negative  \n",
       "4   Neutral  "
      ]
     },
     "execution_count": 3,
=======
       "   Analysis  char_count  ...  stopword_count  unique_word_count  htag_count  \\\n",
       "0  Positive           9  ...               0                  1           0   \n",
       "1   Neutral          40  ...               0                  6           0   \n",
       "2  Positive         218  ...              22                 39           0   \n",
       "3  Positive          44  ...               3                  8           0   \n",
       "4  Positive          67  ...               5                 13           0   \n",
       "\n",
       "   mention_count                                        punct_count  \\\n",
       "0              0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...   \n",
       "1              0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...   \n",
       "2              0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...   \n",
       "3              0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...   \n",
       "4              0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...   \n",
       "\n",
       "   avg_wordlength avg_sentlength  unique_vs_words  stopwords_vs_words  \\\n",
       "0        9.000000           1.00         1.000000            0.000000   \n",
       "1        6.666667           6.00         1.000000            0.000000   \n",
       "2        5.069767          10.75         0.906977            0.511628   \n",
       "3        5.500000           4.00         1.000000            0.375000   \n",
       "4        5.153846          13.00         1.000000            0.384615   \n",
       "\n",
       "                                              tokens  \n",
       "0                                              [lol]  \n",
       "1     [4ch3t3, _syco, hehe, butterfly, issues, 2, 0]  \n",
       "2  [2011, this, is, aapl, trend, line, that, play...  \n",
       "3       [can, yâ, all, give, her, own, pink, iphone]  \n",
       "4  [ios16, is, messing, with, my, phone, service,...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 26,
>>>>>>> 409ddbd (worked on notebook 3)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing tweet data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "trading_hours_tweets = pd.read_csv('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/trading_hours_tweets.csv', encoding='latin-1')\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86941caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80484 entries, 0 to 80483\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Dates         80484 non-null  object \n",
      " 1   Time          80473 non-null  object \n",
      " 2   user          80473 non-null  object \n",
      " 3   likes         80473 non-null  object \n",
      " 4   source        80460 non-null  object \n",
      " 5   text          80460 non-null  object \n",
      " 6   Subjectivity  80447 non-null  float64\n",
      " 7   Polarity      80447 non-null  float64\n",
      " 8   Analysis      80447 non-null  object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "trading_hours_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf2e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:00:52</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing an uptick in chatter on wallstree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>#aapl    #wallstreetbets  #investors https://t...</td>\n",
       "      <td>0.0666666666666666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>19:10:36</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on 4chan over t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>#aapl    #4chan  #investing https://t.co/R9pFh...</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>17:20:19</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on wallstreetbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>#aapl    #wallstreetbets  #stock https://t.co/...</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>13:39:45</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL was the 4th most mentioned on wallstreet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>#aapl    #wallstreetbets  #investing https://t...</td>\n",
       "      <td>0.2833333333333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>17:46:24</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on 4chan over t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>#aapl    #4chan  #investing https://t.co/QomtQ...</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5385</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>14:45:43</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on 4chan over t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>#aapl    #4chan  #daytrading https://t.co/NCuS...</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>14:41:55</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL working its way into the top 10 most men...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7423</th>\n",
       "      <td>#aapl    #wallstreetbets  #stocks https://t.co...</td>\n",
       "      <td>0.3555555555555555</td>\n",
       "      <td>0.3333333333333333</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8962</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>15:38:19</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL was the 4th most mentioned on wallstreet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8963</th>\n",
       "      <td>Via https://t.co/DoXFBxcu94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8964</th>\n",
       "      <td>#aapl    #wallstreetbets  #stockmarket https:/...</td>\n",
       "      <td>0.2833333333333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9110</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>14:58:07</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on wallstreetbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9111</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9112</th>\n",
       "      <td>#aapl    #wallstreetbets  #stocks https://t.co...</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9204</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>14:38:08</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL one of the most mentioned on 4chan over ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9206</th>\n",
       "      <td>#aapl    #4chan  #investors https://t.co/odSTL...</td>\n",
       "      <td>0.2833333333333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9407</th>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>13:48:00</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>0</td>\n",
       "      <td>topstonks</td>\n",
       "      <td>$AAPL seeing sustained chatter on 4chan over t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>Via https://t.co/DoXFBxbWjw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>#aapl    #4chan  #trading https://t.co/4GG2MbSgnK</td>\n",
       "      <td>0.0833333333333333</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57156</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>16:40:06</td>\n",
       "      <td>Aprico2016</td>\n",
       "      <td>0</td>\n",
       "      <td>Aprico</td>\n",
       "      <td>100åã°ããºã§å¯è½ï¼iPhoneã®ç»é¢ãã...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57157</th>\n",
       "      <td>https://t.co/eS1znQWCXF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80406</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:40:06</td>\n",
       "      <td>Aprico2016</td>\n",
       "      <td>0</td>\n",
       "      <td>Aprico</td>\n",
       "      <td>ãéä½¿ç¨ã®Appãåãé¤ãããæ´»ç¨ã...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80407</th>\n",
       "      <td>https://t.co/MjrSORN3Jl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Dates                Time  \\\n",
       "206                                           2022-10-04            20:00:52   \n",
       "207                          Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "208    #aapl    #wallstreetbets  #investors https://t...  0.0666666666666666   \n",
       "544                                           2022-10-04            19:10:36   \n",
       "545                          Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "546    #aapl    #4chan  #investing https://t.co/R9pFh...  0.0833333333333333   \n",
       "1268                                          2022-10-04            17:20:19   \n",
       "1269                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "1270   #aapl    #wallstreetbets  #stock https://t.co/...  0.0833333333333333   \n",
       "2930                                          2022-10-04            13:39:45   \n",
       "2931                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "2932   #aapl    #wallstreetbets  #investing https://t...  0.2833333333333333   \n",
       "4131                                          2022-10-03            17:46:24   \n",
       "4132                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "4133   #aapl    #4chan  #investing https://t.co/QomtQ...  0.0833333333333333   \n",
       "5385                                          2022-10-03            14:45:43   \n",
       "5386                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "5387   #aapl    #4chan  #daytrading https://t.co/NCuS...  0.0833333333333333   \n",
       "7421                                          2022-10-02            14:41:55   \n",
       "7422                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "7423   #aapl    #wallstreetbets  #stocks https://t.co...  0.3555555555555555   \n",
       "8962                                          2022-10-01            15:38:19   \n",
       "8963                         Via https://t.co/DoXFBxcu94                 NaN   \n",
       "8964   #aapl    #wallstreetbets  #stockmarket https:/...  0.2833333333333333   \n",
       "9110                                          2022-10-01            14:58:07   \n",
       "9111                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "9112   #aapl    #wallstreetbets  #stocks https://t.co...  0.0833333333333333   \n",
       "9204                                          2022-10-01            14:38:08   \n",
       "9205                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "9206   #aapl    #4chan  #investors https://t.co/odSTL...  0.2833333333333333   \n",
       "9407                                          2022-10-01            13:48:00   \n",
       "9408                         Via https://t.co/DoXFBxbWjw                 NaN   \n",
       "9409   #aapl    #4chan  #trading https://t.co/4GG2MbSgnK  0.0833333333333333   \n",
       "57156                                         2022-10-04            16:40:06   \n",
       "57157                            https://t.co/eS1znQWCXF                 0.0   \n",
       "80406                                         2022-10-03            16:40:06   \n",
       "80407                            https://t.co/MjrSORN3Jl                 0.0   \n",
       "\n",
       "                     user     likes     source  \\\n",
       "206             topstonks         0  topstonks   \n",
       "207                   NaN       NaN        NaN   \n",
       "208                   0.0   Neutral        NaN   \n",
       "544             topstonks         0  topstonks   \n",
       "545                   NaN       NaN        NaN   \n",
       "546                  -0.1  Negative        NaN   \n",
       "1268            topstonks         0  topstonks   \n",
       "1269                  NaN       NaN        NaN   \n",
       "1270                 -0.1  Negative        NaN   \n",
       "2930            topstonks         0  topstonks   \n",
       "2931                  NaN       NaN        NaN   \n",
       "2932                 0.25  Positive        NaN   \n",
       "4131            topstonks         0  topstonks   \n",
       "4132                  NaN       NaN        NaN   \n",
       "4133                 -0.1  Negative        NaN   \n",
       "5385            topstonks         0  topstonks   \n",
       "5386                  NaN       NaN        NaN   \n",
       "5387                 -0.1  Negative        NaN   \n",
       "7421            topstonks         0  topstonks   \n",
       "7422                  NaN       NaN        NaN   \n",
       "7423   0.3333333333333333  Positive        NaN   \n",
       "8962            topstonks         0  topstonks   \n",
       "8963                  NaN       NaN        NaN   \n",
       "8964                 0.25  Positive        NaN   \n",
       "9110            topstonks         0  topstonks   \n",
       "9111                  NaN       NaN        NaN   \n",
       "9112                 -0.1  Negative        NaN   \n",
       "9204            topstonks         0  topstonks   \n",
       "9205                  NaN       NaN        NaN   \n",
       "9206                 0.25  Positive        NaN   \n",
       "9407            topstonks         0  topstonks   \n",
       "9408                  NaN       NaN        NaN   \n",
       "9409                 -0.1  Negative        NaN   \n",
       "57156          Aprico2016         0     Aprico   \n",
       "57157                 0.0   Neutral        NaN   \n",
       "80406          Aprico2016         0     Aprico   \n",
       "80407                 0.0   Neutral        NaN   \n",
       "\n",
       "                                                    text  Subjectivity  \\\n",
       "206    $AAPL seeing an uptick in chatter on wallstree...           NaN   \n",
       "207                                                  NaN           NaN   \n",
       "208                                                  NaN           NaN   \n",
       "544    $AAPL seeing sustained chatter on 4chan over t...           NaN   \n",
       "545                                                  NaN           NaN   \n",
       "546                                                  NaN           NaN   \n",
       "1268   $AAPL seeing sustained chatter on wallstreetbe...           NaN   \n",
       "1269                                                 NaN           NaN   \n",
       "1270                                                 NaN           NaN   \n",
       "2930   $AAPL was the 4th most mentioned on wallstreet...           NaN   \n",
       "2931                                                 NaN           NaN   \n",
       "2932                                                 NaN           NaN   \n",
       "4131   $AAPL seeing sustained chatter on 4chan over t...           NaN   \n",
       "4132                                                 NaN           NaN   \n",
       "4133                                                 NaN           NaN   \n",
       "5385   $AAPL seeing sustained chatter on 4chan over t...           NaN   \n",
       "5386                                                 NaN           NaN   \n",
       "5387                                                 NaN           NaN   \n",
       "7421   $AAPL working its way into the top 10 most men...           NaN   \n",
       "7422                                                 NaN           NaN   \n",
       "7423                                                 NaN           NaN   \n",
       "8962   $AAPL was the 4th most mentioned on wallstreet...           NaN   \n",
       "8963                                                 NaN           NaN   \n",
       "8964                                                 NaN           NaN   \n",
       "9110   $AAPL seeing sustained chatter on wallstreetbe...           NaN   \n",
       "9111                                                 NaN           NaN   \n",
       "9112                                                 NaN           NaN   \n",
       "9204   $AAPL one of the most mentioned on 4chan over ...           NaN   \n",
       "9205                                                 NaN           NaN   \n",
       "9206                                                 NaN           NaN   \n",
       "9407   $AAPL seeing sustained chatter on 4chan over t...           NaN   \n",
       "9408                                                 NaN           NaN   \n",
       "9409                                                 NaN           NaN   \n",
       "57156  100åã°ããºã§å¯è½ï¼iPhoneã®ç»é¢ãã...           NaN   \n",
       "57157                                                NaN           NaN   \n",
       "80406  ãéä½¿ç¨ã®Appãåãé¤ãããæ´»ç¨ã...           NaN   \n",
       "80407                                                NaN           NaN   \n",
       "\n",
       "       Polarity Analysis  \n",
       "206         NaN      NaN  \n",
       "207         NaN      NaN  \n",
       "208         NaN      NaN  \n",
       "544         NaN      NaN  \n",
       "545         NaN      NaN  \n",
       "546         NaN      NaN  \n",
       "1268        NaN      NaN  \n",
       "1269        NaN      NaN  \n",
       "1270        NaN      NaN  \n",
       "2930        NaN      NaN  \n",
       "2931        NaN      NaN  \n",
       "2932        NaN      NaN  \n",
       "4131        NaN      NaN  \n",
       "4132        NaN      NaN  \n",
       "4133        NaN      NaN  \n",
       "5385        NaN      NaN  \n",
       "5386        NaN      NaN  \n",
       "5387        NaN      NaN  \n",
       "7421        NaN      NaN  \n",
       "7422        NaN      NaN  \n",
       "7423        NaN      NaN  \n",
       "8962        NaN      NaN  \n",
       "8963        NaN      NaN  \n",
       "8964        NaN      NaN  \n",
       "9110        NaN      NaN  \n",
       "9111        NaN      NaN  \n",
       "9112        NaN      NaN  \n",
       "9204        NaN      NaN  \n",
       "9205        NaN      NaN  \n",
       "9206        NaN      NaN  \n",
       "9407        NaN      NaN  \n",
       "9408        NaN      NaN  \n",
       "9409        NaN      NaN  \n",
       "57156       NaN      NaN  \n",
       "57157       NaN      NaN  \n",
       "80406       NaN      NaN  \n",
       "80407       NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_hours_tweets[trading_hours_tweets.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fc1264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:10</td>\n",
       "      <td>BarbaraDarlin</td>\n",
       "      <td>0</td>\n",
       "      <td>Tweetbot for iÎS</td>\n",
       "      <td>.@apple needs to do this WORLDWIDE! https://t....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:03</td>\n",
       "      <td>Options</td>\n",
       "      <td>0</td>\n",
       "      <td>StockTwits Web</td>\n",
       "      <td>#HotOptions Report For End Of Day, October 4, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:59</td>\n",
       "      <td>IGSquawk</td>\n",
       "      <td>8</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:54</td>\n",
       "      <td>breckyunits</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>I gave this talk in 2017 at @forwardJS . They ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-0.366667</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:11</td>\n",
       "      <td>emilesmithjoe</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@Apple make a cannon emoji. Thank you</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80479</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:38:26</td>\n",
       "      <td>itsalphauranium</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@MKBHD Just now i plugged in the charger, my p...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80480</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:38:26</td>\n",
       "      <td>eripaka0629</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>ã¤ã¼ãã¤ï¼ãã¬ã³ãºã®ã¤ã³ãã¼ã·ã...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80481</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:38:24</td>\n",
       "      <td>TopazDeville</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>\"Iphone 14, Iphone 14, Iphone 14\" https://t.co...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80482</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:38:21</td>\n",
       "      <td>MTMhhbxbB54Wkpa</td>\n",
       "      <td>0</td>\n",
       "      <td>ãã®ãã</td>\n",
       "      <td>Amazon è¯ãã£ããè¦ã¦ãã ãã\\n\\nAp...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80483</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>16:38:20</td>\n",
       "      <td>BassamOdaymat</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>@ssiimrran @Asisiya3 @UniverseIce People are c...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80447 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dates      Time             user likes               source  \\\n",
       "0      2022-10-04  20:29:10    BarbaraDarlin     0    Tweetbot for iÎS   \n",
       "1      2022-10-04  20:29:03          Options     0       StockTwits Web   \n",
       "2      2022-10-04  20:27:59         IGSquawk     8            TweetDeck   \n",
       "3      2022-10-04  20:27:54      breckyunits     0      Twitter Web App   \n",
       "4      2022-10-04  20:27:11    emilesmithjoe     0   Twitter for iPhone   \n",
       "...           ...       ...              ...   ...                  ...   \n",
       "80479  2022-10-03  16:38:26  itsalphauranium     0   Twitter for iPhone   \n",
       "80480  2022-10-03  16:38:26      eripaka0629     3   Twitter for iPhone   \n",
       "80481  2022-10-03  16:38:24     TopazDeville     0      Twitter Web App   \n",
       "80482  2022-10-03  16:38:21  MTMhhbxbB54Wkpa     0         ãã®ãã   \n",
       "80483  2022-10-03  16:38:20    BassamOdaymat     0  Twitter for Android   \n",
       "\n",
       "                                                    text  Subjectivity  \\\n",
       "0      .@apple needs to do this WORLDWIDE! https://t....      0.000000   \n",
       "1      #HotOptions Report For End Of Day, October 4, ...      0.000000   \n",
       "2      FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...      0.000000   \n",
       "3      I gave this talk in 2017 at @forwardJS . They ...      0.633333   \n",
       "4                  @Apple make a cannon emoji. Thank you      0.000000   \n",
       "...                                                  ...           ...   \n",
       "80479  @MKBHD Just now i plugged in the charger, my p...      0.677778   \n",
       "80480  ã¤ã¼ãã¤ï¼ãã¬ã³ãºã®ã¤ã³ãã¼ã·ã...      0.000000   \n",
       "80481  \"Iphone 14, Iphone 14, Iphone 14\" https://t.co...      0.000000   \n",
       "80482  Amazon è¯ãã£ããè¦ã¦ãã ãã\\n\\nAp...      0.000000   \n",
       "80483  @ssiimrran @Asisiya3 @UniverseIce People are c...      0.625000   \n",
       "\n",
       "       Polarity  Analysis  \n",
       "0      0.000000   Neutral  \n",
       "1      0.000000   Neutral  \n",
       "2      0.000000   Neutral  \n",
       "3     -0.366667  Negative  \n",
       "4      0.000000   Neutral  \n",
       "...         ...       ...  \n",
       "80479  0.377778  Positive  \n",
       "80480  0.000000   Neutral  \n",
       "80481  0.000000   Neutral  \n",
       "80482  0.000000   Neutral  \n",
       "80483  0.425000  Positive  \n",
       "\n",
       "[80447 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_hours_tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e4e55",
   "metadata": {},
   "source": [
    "## 3.2 Feature Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9943a5",
   "metadata": {},
   "source": [
    "Text Mining is the process of deriving meaningful information from natural language text. Natural Language Processing(NLP) is a part of computer science and artificial intelligence which deals with human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0bc9",
   "metadata": {},
   "source": [
    "### 3.2.1 Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d481c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of characters present in a tweet.\n",
    "def count_chars(text):\n",
    "    return len(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290e95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words present in each line of tweet\n",
    "def count_words(text):\n",
    "    return len(str(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfa557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of punctuation\n",
    "import string\n",
    "def count_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7322666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words in quotation marks\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(('.'|\".\"), text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed13bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79ed6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of unique words\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f11e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c64408a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of stopwords\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53773503",
   "metadata": {},
   "source": [
    "### 3.2.2 Implementation of feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43ac877",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_59516/3629223324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'char_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopword_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_unique_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_59516/3629223324.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'char_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopword_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_unique_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_59516/2946967176.py\u001b[0m in \u001b[0;36mcount_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Count the number of words present in each line of tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "trading_hours_tweets['char_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_chars(x))\n",
    "trading_hours_tweets['word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_words(x))\n",
    "trading_hours_tweets['sent_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_sent(x))\n",
    "trading_hours_tweets['stopword_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_stopwords(x))\n",
    "trading_hours_tweets['unique_word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_unique_words(x))\n",
    "trading_hours_tweets['htag_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_htags(x))\n",
    "trading_hours_tweets['mention_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_mentions(x))\n",
    "trading_hours_tweets['punct_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average word length\n",
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average sentence length\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63271f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of unique words to total word count\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of stopwords to total word count\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b359c",
   "metadata": {},
   "source": [
    "### 3.2.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca02d3",
   "metadata": {},
   "source": [
    "Here we turn our twitter strings to lists of individual tokens (words, punctuations). Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e93ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#NLTK tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "trading_hours_tweets['tokens'] = trading_hours_tweets['text'].apply(tokenizer.tokenize)\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19d910",
   "metadata": {},
   "source": [
    "### 3.2.4 Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c664ea",
   "metadata": {},
   "source": [
    "Using the tokens that we generated, we want to explore sentence length and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 27,
>>>>>>> 409ddbd (worked on notebook 3)
   "id": "c1ef613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(tokens) for tokens in trading_hours_tweets['tokens']]\n",
    "vocab = sorted(list(set([word for tokens in trading_hours_tweets['tokens'] for word in tokens])))\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.xlabel('Sentence Length (in words)')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Sentence Lengths')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 28,
>>>>>>> 409ddbd (worked on notebook 3)
   "id": "2e193728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('We have a vocabulary size of', len(vocab), 'unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d91778",
   "metadata": {},
   "source": [
    "### 3.2.5 Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c4cb3",
   "metadata": {},
   "source": [
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 29,
>>>>>>> 409ddbd (worked on notebook 3)
   "id": "d4efd8c7",
   "metadata": {
    "scrolled": true
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>char_count</th>\n",
       "      <th>...</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>avg_sentlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tweet_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:52</td>\n",
       "      <td>nicrae45</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.800</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[lol]</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:00</td>\n",
       "      <td>0x1585D65F0</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>4ch3t3 _syco   hehe butterfly issues 2.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[4ch3t3, _syco, hehe, butterfly, issues, 2, 0]</td>\n",
       "      <td>4ch3t3 _syco hehe butterfly issues 2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:34</td>\n",
       "      <td>equitydd</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2011  this is $aapl trend line that played out...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Positive</td>\n",
       "      <td>218</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.069767</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>[2011, this, is, aapl, trend, line, that, play...</td>\n",
       "      <td>2011 $aapl trend line played summer rally, pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:31</td>\n",
       "      <td>THESMARR</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>can yâall give   her own pink iphone ???</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>[can, yâ, all, give, her, own, pink, iphone]</td>\n",
       "      <td>yâall give pink iphone ???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:26</td>\n",
       "      <td>_Idontknowbro_</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>ios16 is messing with my phone service yâal...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.153846</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[ios16, is, messing, with, my, phone, service,...</td>\n",
       "      <td>ios16 messing phone service yâall need fix fast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time            user  likes              source  \\\n",
       "0  2022-10-30  20:29:52        nicrae45      0  Twitter for iPhone   \n",
       "1  2022-10-30  20:29:00     0x1585D65F0      1  Twitter for iPhone   \n",
       "2  2022-10-30  20:28:34        equitydd      1  Twitter for iPhone   \n",
       "3  2022-10-30  20:28:31        THESMARR      0     Twitter Web App   \n",
       "4  2022-10-30  20:28:26  _Idontknowbro_      0  Twitter for iPhone   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0                                                lol         0.700     0.800   \n",
       "1           4ch3t3 _syco   hehe butterfly issues 2.0         0.000     0.000   \n",
       "2  2011  this is $aapl trend line that played out...         0.325     0.325   \n",
       "3         can yâall give   her own pink iphone ???         0.650     0.250   \n",
       "4   ios16 is messing with my phone service yâal...         0.600     0.200   \n",
       "\n",
       "   Analysis  char_count  ...  unique_word_count  htag_count  mention_count  \\\n",
       "0  Positive           9  ...                  1           0              0   \n",
       "1   Neutral          40  ...                  6           0              0   \n",
       "2  Positive         218  ...                 39           0              0   \n",
       "3  Positive          44  ...                  8           0              0   \n",
       "4  Positive          67  ...                 13           0              0   \n",
       "\n",
       "                                         punct_count  avg_wordlength  \\\n",
       "0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        9.000000   \n",
       "1  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        6.666667   \n",
       "2  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.069767   \n",
       "3  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.500000   \n",
       "4  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.153846   \n",
       "\n",
       "   avg_sentlength unique_vs_words  stopwords_vs_words  \\\n",
       "0            1.00        1.000000            0.000000   \n",
       "1            6.00        1.000000            0.000000   \n",
       "2           10.75        0.906977            0.511628   \n",
       "3            4.00        1.000000            0.375000   \n",
       "4           13.00        1.000000            0.384615   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                              [lol]   \n",
       "1     [4ch3t3, _syco, hehe, butterfly, issues, 2, 0]   \n",
       "2  [2011, this, is, aapl, trend, line, that, play...   \n",
       "3       [can, yâ, all, give, her, own, pink, iphone]   \n",
       "4  [ios16, is, messing, with, my, phone, service,...   \n",
       "\n",
       "                             tweet_without_stopwords  \n",
       "0                                                lol  \n",
       "1             4ch3t3 _syco hehe butterfly issues 2.0  \n",
       "2  2011 $aapl trend line played summer rally, pen...  \n",
       "3                       yâall give pink iphone ???  \n",
       "4  ios16 messing phone service yâall need fix fast  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 409ddbd (worked on notebook 3)
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "trading_hours_tweets['tweet_without_stopwords'] = trading_hours_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437f28e",
   "metadata": {},
   "source": [
    "### 3.2.6 Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508c09c",
   "metadata": {},
   "source": [
    "In simpler terms, lemmitization is the process of converting a word to its base form. Lemmatization considers the context and converts the word to its meaningful base form. For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care'. This is extremely valuable because we want to identify key words that lead to negative, positive and neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 30,
>>>>>>> 409ddbd (worked on notebook 3)
   "id": "228ecd91",
   "metadata": {
    "scrolled": true
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>char_count</th>\n",
       "      <th>...</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>avg_sentlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tweet_without_stopwords</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:52</td>\n",
       "      <td>nicrae45</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>lol</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.800</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[lol]</td>\n",
       "      <td>lol</td>\n",
       "      <td>[lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:29:00</td>\n",
       "      <td>0x1585D65F0</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>4ch3t3 _syco   hehe butterfly issues 2.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[4ch3t3, _syco, hehe, butterfly, issues, 2, 0]</td>\n",
       "      <td>4ch3t3 _syco hehe butterfly issues 2.0</td>\n",
       "      <td>[4ch3t3, _syco, hehe, butterfly, issue, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:34</td>\n",
       "      <td>equitydd</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2011  this is $aapl trend line that played out...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Positive</td>\n",
       "      <td>218</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.069767</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>[2011, this, is, aapl, trend, line, that, play...</td>\n",
       "      <td>2011 $aapl trend line played summer rally, pen...</td>\n",
       "      <td>[2011, $aapl, trend, line, played, summer, ral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:31</td>\n",
       "      <td>THESMARR</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>can yâall give   her own pink iphone ???</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>[can, yâ, all, give, her, own, pink, iphone]</td>\n",
       "      <td>yâall give pink iphone ???</td>\n",
       "      <td>[yâall, give, pink, iphone, ???]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-30</td>\n",
       "      <td>20:28:26</td>\n",
       "      <td>_Idontknowbro_</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>ios16 is messing with my phone service yâal...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.200</td>\n",
       "      <td>Positive</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.153846</td>\n",
       "      <td>13.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>[ios16, is, messing, with, my, phone, service,...</td>\n",
       "      <td>ios16 messing phone service yâall need fix fast</td>\n",
       "      <td>[ios16, messing, phone, service, yâall, need...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time            user  likes              source  \\\n",
       "0  2022-10-30  20:29:52        nicrae45      0  Twitter for iPhone   \n",
       "1  2022-10-30  20:29:00     0x1585D65F0      1  Twitter for iPhone   \n",
       "2  2022-10-30  20:28:34        equitydd      1  Twitter for iPhone   \n",
       "3  2022-10-30  20:28:31        THESMARR      0     Twitter Web App   \n",
       "4  2022-10-30  20:28:26  _Idontknowbro_      0  Twitter for iPhone   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0                                                lol         0.700     0.800   \n",
       "1           4ch3t3 _syco   hehe butterfly issues 2.0         0.000     0.000   \n",
       "2  2011  this is $aapl trend line that played out...         0.325     0.325   \n",
       "3         can yâall give   her own pink iphone ???         0.650     0.250   \n",
       "4   ios16 is messing with my phone service yâal...         0.600     0.200   \n",
       "\n",
       "   Analysis  char_count  ...  htag_count  mention_count  \\\n",
       "0  Positive           9  ...           0              0   \n",
       "1   Neutral          40  ...           0              0   \n",
       "2  Positive         218  ...           0              0   \n",
       "3  Positive          44  ...           0              0   \n",
       "4  Positive          67  ...           0              0   \n",
       "\n",
       "                                         punct_count  avg_wordlength  \\\n",
       "0  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        9.000000   \n",
       "1  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        6.666667   \n",
       "2  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.069767   \n",
       "3  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.500000   \n",
       "4  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.153846   \n",
       "\n",
       "   avg_sentlength  unique_vs_words stopwords_vs_words  \\\n",
       "0            1.00         1.000000           0.000000   \n",
       "1            6.00         1.000000           0.000000   \n",
       "2           10.75         0.906977           0.511628   \n",
       "3            4.00         1.000000           0.375000   \n",
       "4           13.00         1.000000           0.384615   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                              [lol]   \n",
       "1     [4ch3t3, _syco, hehe, butterfly, issues, 2, 0]   \n",
       "2  [2011, this, is, aapl, trend, line, that, play...   \n",
       "3       [can, yâ, all, give, her, own, pink, iphone]   \n",
       "4  [ios16, is, messing, with, my, phone, service,...   \n",
       "\n",
       "                             tweet_without_stopwords  \\\n",
       "0                                                lol   \n",
       "1             4ch3t3 _syco hehe butterfly issues 2.0   \n",
       "2  2011 $aapl trend line played summer rally, pen...   \n",
       "3                       yâall give pink iphone ???   \n",
       "4  ios16 messing phone service yâall need fix fast   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0                                              [lol]  \n",
       "1       [4ch3t3, _syco, hehe, butterfly, issue, 2.0]  \n",
       "2  [2011, $aapl, trend, line, played, summer, ral...  \n",
       "3                 [yâall, give, pink, iphone, ???]  \n",
       "4  [ios16, messing, phone, service, yâall, need...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 409ddbd (worked on notebook 3)
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "trading_hours_tweets['tweet_lemmatized'] = trading_hours_tweets['tweet_without_stopwords'].apply(lemmatize_text)\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "918944b2",
   "metadata": {},
   "source": [
    "## 3.3 TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b439685",
   "metadata": {},
   "source": [
    "We will first try the TF-IDF method. TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7f9ca",
   "metadata": {},
   "source": [
    "### 3.3.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769a26",
   "metadata": {},
   "source": [
    "Performing a simple pre-processing step, like removing links, removing user name, numbers, double space, punctuation, lower casing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "my_punctuation = string.punctuation\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+ my_punctuation + ']+',' ', sent) # strip punctuation\n",
    "    sent = re.sub('s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data random state 0 and test_size 0.25 default as you did not give the test_size\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trading_hours_tweets[['Rejoined_Lemmatize']], df['Product'], random_state = 0)\n",
    "\n",
    "# get the params\n",
    "tfidf_params = dict(sublinear_tf= True, \n",
    "                       min_df = 5, \n",
    "                       norm= 'l2', \n",
    "                       ngram_range= (1,2), \n",
    "                       stop_words ='english')\n",
    "\n",
    "# create a Pipeline that will do features transformation then pass to the model\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "('features', TfidfVectorizer(**tfidf_params)),\n",
    "('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f51de",
   "metadata": {},
   "source": [
    "## 3.4 Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39574e41",
   "metadata": {},
   "source": [
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, they are vector representations of a particular word. In this project, we will use Word2Vec embedding method to map a word to a fixed-length vector. In simple terms, words that have similar meanings or are related closely, when mapped into a vector space would appear closer, like in a cluster. This can help us understand the semantics of the words in a sentence better than any previously mentioned technique. \n",
    "\n",
    "Word2Vec utilizes two architectures :\n",
    "\n",
    "**CBOW (Continuous Bag of Words)**: CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. \n",
    "\n",
    "**Skip Gram** : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "\n",
    "Challenges \\\n",
    "Word2Vec cannot handle out of vocabulary words very well. \\\n",
    "Doesn’t take the context of the word into account. \\ \n",
    "Requires fairly large corpus to train on. \\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa679d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('averaged_perceptron_tagger'); nltk.download('punkt')"
=======
   "id": "d9914f27",
   "metadata": {},
   "source": [
    "## 3.3 TF-IDF "
>>>>>>> 409ddbd (worked on notebook 3)
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "76dd4d1a",
=======
   "id": "58d4b607",
   "metadata": {},
   "source": [
    "We will first try the TF-IDF method. TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d9bd3",
   "metadata": {},
   "source": [
    "### 3.3.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916abec",
   "metadata": {},
   "source": [
    "Performing a simple pre-processing step, like removing links, removing user name, numbers, double space, punctuation, lower casing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4381b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "my_punctuation = string.punctuation\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+ my_punctuation + ']+',' ', sent) # strip punctuation\n",
    "    sent = re.sub('s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37f129",
>>>>>>> 409ddbd (worked on notebook 3)
   "metadata": {},
   "source": [
    "### 3.4.1 Data Preparation"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "id": "6df97a46",
   "metadata": {},
   "source": [
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, they are vector representations of a particular word. In this project, we will use Word2Vec embedding method to map a word to a fixed-length vector. In simple terms, words that have similar meanings or are related closely, when mapped into a vector space would appear closer, like in a cluster. This can help us understand the semantics of the words in a sentence better than any previously mentioned technique. \n",
    "\n",
    "Word2Vec utilizes two architectures :\n",
    "\n",
    "**CBOW (Continuous Bag of Words)**: CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. \n",
    "\n",
    "**Skip Gram** : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "\n",
    "Challenges \\\n",
    "Word2Vec cannot handle out of vocabulary words very well. \\\n",
    "Doesn’t take the context of the word into account. \\ \n",
    "Requires fairly large corpus to train on. \\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa679d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('averaged_perceptron_tagger'); nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63060a4",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9795589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Read data from the file\n",
    "\n",
    "def get_file_data(stop_word_removal='no'):\n",
    "    file_contents = []\n",
    "    with open('trading_hours_tweets') as f:\n",
    "        file_contents = f.read() #Read contents of the text file to a list\n",
    "    text = []\n",
    "    for val in file_contents.split('.'):\n",
    "        sent = re.findall(\"[A-Za-z]+\", val) #Keep only the alphabets and remove everything else from each line\n",
    "        line = ''\n",
    "        for words in sent:\n",
    "            \n",
    "            if stop_word_removal == 'yes': #Iterate each word in the sentence and remove stopwords if specified\n",
    "                if len(words) > 1 and words not in stop_words:\n",
    "                    line = line + ' ' + words\n",
    "            else:\n",
    "                if len(words) > 1 :\n",
    "                    line = line + ' ' + words\n",
    "        text.append(line)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Generate variables\n",
    "\n",
    "def generate_dictionary_data(text):\n",
    "    word_to_index= dict()\n",
    "    index_to_word = dict()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    vocab_size = 0\n",
    "    \n",
    "    for row in text:\n",
    "        for word in row.split():\n",
    "            word = word.lower() #Convert each word to lowercase\n",
    "            corpus.append(word)\n",
    "            if word_to_index.get(word) == None: #\n",
    "                word_to_index.update ( {word : count})\n",
    "                index_to_word.update ( {count : word })\n",
    "                count  += 1 #Update the dictionaries with the word and there counts if the word is not already present in the dictionary\n",
    "    vocab_size = len(word_to_index)\n",
    "    length_of_corpus = len(corpus)\n",
    "    \n",
    "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30584872",
   "metadata": {},
   "source": [
    "Variable definitions:\n",
    "\n",
    "word_to_index : A dictionary mapping each word to an integer value {‘modern’: 0, ‘humans’: 1} \\\n",
    "index_to_word : A dictionary mapping each integer value to a word {0: ‘modern’, 1: ‘humans’} \\\n",
    "corpus : The entire data consisting of all the words \\\n",
    "vocab_size : Number of unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Generate Training Data\n",
    "\n",
    "\n",
    "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index):\n",
    "    \n",
    "    #Create an array of size = vocab_size filled with zeros\n",
    "    trgt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    #Get the index of the target_word according to the dictionary word_to_index. \n",
    "    #If target_word = best, the index according to the dictionary word_to_index is 0. \n",
    "    #So the one hot vector will be [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    index_of_word_dictionary = word_to_index.get(target_word) \n",
    "    \n",
    "    #Set the index to 1\n",
    "    trgt_word_vector[index_of_word_dictionary] = 1\n",
    "    \n",
    "    #Repeat same steps for context_words but in a loop\n",
    "    ctxt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    \n",
    "    for word in context_words:\n",
    "        index_of_word_dictionary = word_to_index.get(word) \n",
    "        ctxt_word_vector[index_of_word_dictionary] = 1\n",
    "        \n",
    "    return trgt_word_vector,ctxt_word_vector"
   ]
  },
  {
>>>>>>> 409ddbd (worked on notebook 3)
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96fcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a53be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948606da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e70d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2957d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c04c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bab245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76984f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf788bd",
   "metadata": {},
   "source": [
    "# 3 Pre-Processing and Training Data<a id='4_Pre-Processing_and_Training_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85953d52",
   "metadata": {},
   "source": [
    "The objectives of this notebook is to create a machine learning capable to segmentize a given Tweet into its category. We will be using word embedding to define the clusters of the tweets. Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions.\n",
    "\n",
    "Guidance from Springboard:\n",
    "* Create dummy or indicator features for categorical variables\n",
    "* Standardize the magnitude of numeric features using a scaler\n",
    "* Split your data into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dcbe6",
   "metadata": {},
   "source": [
    "## 3.1 Imports<a id='4.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a928531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a2e4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>Volume</th>\n",
       "      <th>%_change_Open</th>\n",
       "      <th>%_change_High</th>\n",
       "      <th>%_change_Low</th>\n",
       "      <th>%_change_Close</th>\n",
       "      <th>%_change_Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>114311700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>87830100</td>\n",
       "      <td>4.934514</td>\n",
       "      <td>2.201715</td>\n",
       "      <td>4.771583</td>\n",
       "      <td>2.562311</td>\n",
       "      <td>-23.166133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>79471000</td>\n",
       "      <td>-0.661926</td>\n",
       "      <td>0.793328</td>\n",
       "      <td>-0.866491</td>\n",
       "      <td>0.205326</td>\n",
       "      <td>-9.517352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>68402200</td>\n",
       "      <td>1.207739</td>\n",
       "      <td>0.108555</td>\n",
       "      <td>1.545351</td>\n",
       "      <td>-0.662562</td>\n",
       "      <td>-13.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>85925600</td>\n",
       "      <td>-2.242648</td>\n",
       "      <td>-3.009345</td>\n",
       "      <td>-3.973285</td>\n",
       "      <td>-3.671873</td>\n",
       "      <td>25.618182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time     Volume  %_change_Open  %_change_High  \\\n",
       "0  2022-10-03  00:00:00  114311700            NaN            NaN   \n",
       "1  2022-10-04  00:00:00   87830100       4.934514       2.201715   \n",
       "2  2022-10-05  00:00:00   79471000      -0.661926       0.793328   \n",
       "3  2022-10-06  00:00:00   68402200       1.207739       0.108555   \n",
       "4  2022-10-07  00:00:00   85925600      -2.242648      -3.009345   \n",
       "\n",
       "   %_change_Low  %_change_Close  %_change_Volume  \n",
       "0           NaN             NaN              NaN  \n",
       "1      4.771583        2.562311       -23.166133  \n",
       "2     -0.866491        0.205326        -9.517352  \n",
       "3      1.545351       -0.662562       -13.928100  \n",
       "4     -3.973285       -3.671873        25.618182  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing stock data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "eda_stock_data = pd.read_csv('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/eda_stock_data.csv', encoding='latin-1')\n",
    "eda_stock_data.reset_index(drop=True, inplace=True)\n",
    "eda_stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a92369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Time</th>\n",
       "      <th>user</th>\n",
       "      <th>likes</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:10</td>\n",
       "      <td>BarbaraDarlin</td>\n",
       "      <td>0</td>\n",
       "      <td>Tweetbot for iÎS</td>\n",
       "      <td>.@apple needs to do this WORLDWIDE! https://t....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:29:03</td>\n",
       "      <td>Options</td>\n",
       "      <td>0</td>\n",
       "      <td>StockTwits Web</td>\n",
       "      <td>#HotOptions Report For End Of Day, October 4, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:59</td>\n",
       "      <td>IGSquawk</td>\n",
       "      <td>8</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:54</td>\n",
       "      <td>breckyunits</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>I gave this talk in 2017 at @forwardJS . They ...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>-0.366667</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>20:27:11</td>\n",
       "      <td>emilesmithjoe</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>@Apple make a cannon emoji. Thank you</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dates      Time           user likes              source  \\\n",
       "0  2022-10-04  20:29:10  BarbaraDarlin     0   Tweetbot for iÎS   \n",
       "1  2022-10-04  20:29:03        Options     0      StockTwits Web   \n",
       "2  2022-10-04  20:27:59       IGSquawk     8           TweetDeck   \n",
       "3  2022-10-04  20:27:54    breckyunits     0     Twitter Web App   \n",
       "4  2022-10-04  20:27:11  emilesmithjoe     0  Twitter for iPhone   \n",
       "\n",
       "                                                text  Subjectivity  Polarity  \\\n",
       "0  .@apple needs to do this WORLDWIDE! https://t....      0.000000  0.000000   \n",
       "1  #HotOptions Report For End Of Day, October 4, ...      0.000000  0.000000   \n",
       "2  FANG+ Constituents:\\n\\n$AAPL 146.04 +2.52%\\n$A...      0.000000  0.000000   \n",
       "3  I gave this talk in 2017 at @forwardJS . They ...      0.633333 -0.366667   \n",
       "4              @Apple make a cannon emoji. Thank you      0.000000  0.000000   \n",
       "\n",
       "   Analysis  \n",
       "0   Neutral  \n",
       "1   Neutral  \n",
       "2   Neutral  \n",
       "3  Negative  \n",
       "4   Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing tweet data from previous notebook \"02_Exploratory_Data_Analysis\"\n",
    "trading_hours_tweets = pd.read_csv('/Users/user/Documents/Springboard_Data_Science/Capstone_2_Twitter_Sentiment_Analysis/Data/trading_hours_tweets.csv', encoding='latin-1')\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e4e55",
   "metadata": {},
   "source": [
    "## 3.2 Feature Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9943a5",
   "metadata": {},
   "source": [
    "Text Mining is the process of deriving meaningful information from natural language text. Natural Language Processing(NLP) is a part of computer science and artificial intelligence which deals with human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0bc9",
   "metadata": {},
   "source": [
    "### 3.2.1 Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d481c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of characters present in a tweet.\n",
    "def count_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290e95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words present in each line of tweet\n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of punctuation\n",
    "import string\n",
    "def count_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7322666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of words in quotation marks\n",
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(('.'|\".\"), text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed13bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79ed6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of unique words\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f11e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of hashtags\n",
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64408a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of mentions\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "218ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of stopwords\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53773503",
   "metadata": {},
   "source": [
    "### 3.2.2 Implementation of feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43ac877",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_11007/3629223324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'char_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopword_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_unique_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_11007/3629223324.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'char_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopword_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrading_hours_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcount_unique_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/zfqmnfxs5z3dkfs5cn70l8bh0000gn/T/ipykernel_11007/3111555123.py\u001b[0m in \u001b[0;36mcount_chars\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Count the number of characters present in a tweet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "trading_hours_tweets['char_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_chars(x))\n",
    "trading_hours_tweets['word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_words(x))\n",
    "trading_hours_tweets['sent_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_sent(x))\n",
    "trading_hours_tweets['stopword_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_stopwords(x))\n",
    "trading_hours_tweets['unique_word_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_unique_words(x))\n",
    "trading_hours_tweets['htag_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_htags(x))\n",
    "trading_hours_tweets['mention_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_mentions(x))\n",
    "trading_hours_tweets['punct_count'] = trading_hours_tweets[\"text\"].apply(lambda x:count_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average word length\n",
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of average sentence length\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63271f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of unique words to total word count\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of stopwords to total word count\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_hours_tweets['avg_wordlength'] = trading_hours_tweets['char_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['avg_sentlength'] = trading_hours_tweets['word_count']/trading_hours_tweets['sent_count']\n",
    "trading_hours_tweets['unique_vs_words'] = trading_hours_tweets['unique_word_count']/trading_hours_tweets['word_count']\n",
    "trading_hours_tweets['stopwords_vs_words'] = trading_hours_tweets['stopword_count']/trading_hours_tweets['word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b359c",
   "metadata": {},
   "source": [
    "### 3.2.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca02d3",
   "metadata": {},
   "source": [
    "Here we turn our twitter strings to lists of individual tokens (words, punctuations). Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e93ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#NLTK tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "trading_hours_tweets['tokens'] = trading_hours_tweets['text'].apply(tokenizer.tokenize)\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a19d910",
   "metadata": {},
   "source": [
    "### 3.2.4 Sentence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c664ea",
   "metadata": {},
   "source": [
    "Using the tokens that we generated, we want to explore sentence length and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(tokens) for tokens in trading_hours_tweets['tokens']]\n",
    "vocab = sorted(list(set([word for tokens in trading_hours_tweets['tokens'] for word in tokens])))\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.xlabel('Sentence Length (in words)')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Sentence Lengths')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e193728",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('We have a vocabulary size of', len(vocab), 'unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d91778",
   "metadata": {},
   "source": [
    "### 3.2.5 Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c4cb3",
   "metadata": {},
   "source": [
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efd8c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "trading_hours_tweets['tweet_without_stopwords'] = trading_hours_tweets['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437f28e",
   "metadata": {},
   "source": [
    "### 3.2.6 Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508c09c",
   "metadata": {},
   "source": [
    "In simpler terms, lemmitization is the process of converting a word to its base form. Lemmatization considers the context and converts the word to its meaningful base form. For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care'. This is extremely valuable because we want to identify key words that lead to negative, positive and neutral sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ecd91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "trading_hours_tweets['tweet_lemmatized'] = trading_hours_tweets['tweet_without_stopwords'].apply(lemmatize_text)\n",
    "trading_hours_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918944b2",
   "metadata": {},
   "source": [
    "## 3.3 TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b439685",
   "metadata": {},
   "source": [
    "We will first try the TF-IDF method. TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify words in a set of documents. We generally compute a score for each word to signify its importance in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7f9ca",
   "metadata": {},
   "source": [
    "### 3.3.1 Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769a26",
   "metadata": {},
   "source": [
    "Performing a simple pre-processing step, like removing links, removing user name, numbers, double space, punctuation, lower casing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "my_punctuation = string.punctuation\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+ my_punctuation + ']+',' ', sent) # strip punctuation\n",
    "    sent = re.sub('s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "trading_hours_tweets['clean_text'] = trading_hours_tweets['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data random state 0 and test_size 0.25 default as you did not give the test_size\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trading_hours_tweets[['Rejoined_Lemmatize']], df['Product'], random_state = 0)\n",
    "\n",
    "# get the params\n",
    "tfidf_params = dict(sublinear_tf= True, \n",
    "                       min_df = 5, \n",
    "                       norm= 'l2', \n",
    "                       ngram_range= (1,2), \n",
    "                       stop_words ='english')\n",
    "\n",
    "# create a Pipeline that will do features transformation then pass to the model\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "('features', TfidfVectorizer(**tfidf_params)),\n",
    "('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Use clf as a model, fit X_train and y_train\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "# predicted \n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f51de",
   "metadata": {},
   "source": [
    "## 3.4 Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39574e41",
   "metadata": {},
   "source": [
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. Loosely speaking, they are vector representations of a particular word. In this project, we will use Word2Vec embedding method to map a word to a fixed-length vector. In simple terms, words that have similar meanings or are related closely, when mapped into a vector space would appear closer, like in a cluster. This can help us understand the semantics of the words in a sentence better than any previously mentioned technique. \n",
    "\n",
    "Word2Vec utilizes two architectures :\n",
    "\n",
    "**CBOW (Continuous Bag of Words)**: CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. \n",
    "\n",
    "**Skip Gram** : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "\n",
    "Challenges \\\n",
    "Word2Vec cannot handle out of vocabulary words very well. \\\n",
    "Doesn’t take the context of the word into account. \\ \n",
    "Requires fairly large corpus to train on. \\ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa679d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('averaged_perceptron_tagger'); nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd4d1a",
   "metadata": {},
   "source": [
    "### 3.4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96fcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a53be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948606da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e70d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2957d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c04c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bab245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76984f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
